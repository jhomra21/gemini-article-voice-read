import { GoogleGenAI } from '@google/genai';
import type { 
  GenerateContentStreamConfigTTS, 
  SpeechConfigJS,
  VoiceConfigJS,
  PrebuiltVoiceConfigJS,
  SpeakerMode,
  MultiSpeakerVoiceConfigJS,
  Speaker
} from './types';
import { TTS_MODELS, VOICE_OPTIONS } from './types';
import { base64ToUint8Array } from './audio-utils';

export interface TextToSpeechRequest {
  text: string;
  voiceId?: string;
  speakerMode?: SpeakerMode;
  speakers?: Speaker[];
  temperature?: number;
}

export class GeminiService {
  private readonly ai: GoogleGenAI;
  
  constructor(apiKey: string) {
    this.ai = new GoogleGenAI({ apiKey });
  }
  
  /**
   * Generate speech from text using Gemini API
   */
  async generateSpeech(request: TextToSpeechRequest): Promise<ReadableStream<Uint8Array>> {
    const { text, voiceId, speakerMode = 'single', speakers = [], temperature = 1 } = request;
    
    // Validate the text input
    if (!text || text.trim().length === 0) {
      throw new Error('Empty text provided for speech generation');
    }
    
    // Configure speech settings based on speaker mode
    const speechConfig: SpeechConfigJS = this.createSpeechConfig(voiceId, speakerMode, speakers);
    
    // Create the config for the API request
    const config: GenerateContentStreamConfigTTS = {
      responseModalities: ['AUDIO'],
      speechConfig,
      temperature,
    };
    
    // Select the TTS model
    const model = TTS_MODELS.FLASH;
    
    // Create the content for the API request
    const contents = [
      {
        role: 'user',
        parts: [{ text }],
      },
    ];
    
    console.log(`Making TTS API request for text: "${text.substring(0, 30)}..."`);
    console.log('Using model:', model);
    console.log('Using voice:', voiceId || VOICE_OPTIONS[0].id);

    try {
      // Make the API request
      const response = await this.ai.models.generateContentStream({
        model,
        config,
        contents,
      });
      
      // Following Google's documentation for creating WAV files from TTS output
      const stream = new ReadableStream<Uint8Array>({
        async start(controller) {
          try {
            let hasReceivedAudio = false;
            
            // Process the audio chunks as they come in
            for await (const chunk of response) {
              if (!chunk.candidates?.[0]?.content?.parts?.[0]?.inlineData) {
                continue;
              }
              
              const part = chunk.candidates[0].content.parts[0];
              
              if (part.inlineData) {
                const { data } = part.inlineData;
                if (data) {
                  // Convert base64 data to Uint8Array
                  const audioBuffer = base64ToUint8Array(data);
                  controller.enqueue(audioBuffer);
                  hasReceivedAudio = true;
                }
              }
            }
            
            // Check if we received any audio data
            if (!hasReceivedAudio) {
              throw new Error('No audio data was generated by the API. The model may have rejected the input text.');
            }
            
            controller.close();
          } catch (error) {
            console.error('Error processing TTS response:', error);
            controller.error(error);
          }
        }
      });
      
      return stream;
    } catch (error: any) {
      // Enhance error information for common API issues
      if (error?.status === 429) {
        throw new Error('Google API rate limit exceeded. Try again later or upgrade your API plan.');
      } else if (error?.status === 400) {
        throw new Error(`API request rejected: ${error.message || 'Invalid parameters or content policy violation'}`);
      } else if (error?.status === 403) {
        throw new Error('API access denied. Check your API key or permissions.');
      }
      
      console.error('Error calling Gemini API:', error);
      throw error;
    }
  }
  
  /**
   * Create speech configuration based on speaker mode and selected voices
   */
  private createSpeechConfig(
    voiceId?: string, 
    speakerMode: SpeakerMode = 'single',
    speakers: Speaker[] = []
  ): SpeechConfigJS {
    // Default to the first voice if not specified
    const defaultVoiceId = VOICE_OPTIONS[0].id;
    const selectedVoiceId = voiceId || defaultVoiceId;
    
    // For single speaker mode
    if (speakerMode === 'single') {
      const voiceConfig: VoiceConfigJS = {
        prebuiltVoiceConfig: {
          voiceName: selectedVoiceId,
        },
      };
      
      return { voiceConfig };
    }
    
    // For multi-speaker mode
    const speakerVoiceConfigs = speakers.map(speaker => {
      return {
        speaker: speaker.nameInPrompt,
        voiceConfig: {
          prebuiltVoiceConfig: {
            voiceName: speaker.voiceId,
          },
        },
      };
    });
    
    const multiSpeakerVoiceConfig: MultiSpeakerVoiceConfigJS = {
      speakerVoiceConfigs,
    };
    
    return { multiSpeakerVoiceConfig };
  }
} 